# -*- coding: utf-8 -*-
"""pdf-q-a-notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/bhattbhavesh91/pdf-q-a-llamaindex-llama2/blob/main/pdf-q-a-notebook.ipynb

## Chat with your PDF files using LlamaIndex, Astra DB (Apache Cassandra), and Gradient's open-source models, including LLama2 and Streamlit, all designed for seamless interaction with PDF files.

[**Link to my YouTube Channel**](https://www.youtube.com/BhaveshBhatt8791?sub_confirmation=1)

Click on the link below to open a Colab version of the notebook. You will be able to create your own version.

<a href="https://colab.research.google.com/github/bhattbhavesh91//pdf-q-a-llamaindex-llama2/blob/main/pdf-q-a-notebook.ipynb" target="_blank"><img height="40" alt="Run your own notebook in Colab" src = "https://colab.research.google.com/assets/colab-badge.svg"></a>

# Installation
"""

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

loader = PyPDFLoader("the-great-gatsby.pdf")
pages = loader.load_and_split()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500)
texts = text_splitter.split_documents(pages)
print(len(texts))
for i, doc in enumerate(texts[190:193]):
    print(f"--- Chunk {i+1} ---")
    print(doc.page_content[:999])  # print first 500 characters of each
    print()

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"}
)

db = FAISS.from_documents(texts, embeddings)
retriever = db.as_retriever(search_kwargs={"k": 3})

from langchain.llms.base import LLM
from typing import Optional, List, Mapping, Any
import requests
import os

# Set your API key first
os.environ["OPENROUTER_API_KEY"] = "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"

class OpenRouterLLM(LLM):
    model: str = "anthropic/claude-3-haiku:beta"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        headers = {
            "Authorization": f"Bearer {os.environ['OPENROUTER_API_KEY']}",
            "Content-Type": "application/json",
        }
        body = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
        }
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=body
        )
        data = response.json()
        return data["choices"][0]["message"]["content"]

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"model": self.model}

    @property
    def _llm_type(self) -> str:
        return "openrouter"

# import requests
# import os

# headers = {
#     "Authorization": f"Bearer {os.environ['OPENROUTER_API_KEY']}",
# }
# response = requests.get("https://openrouter.ai/api/v1/models", headers=headers)

# models = response.json().get("data", [])
# for model in models:
#     print("-", model.get("id"))

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=OpenRouterLLM(),
    retriever=retriever,
    return_source_documents=True
)

query = "What is name of the book about"
response = qa_chain(query)

print("ðŸ“˜ Answer:", response["result"])
print("\nðŸ“„ Source snippet:\n", response["source_documents"][0].page_content[:400])